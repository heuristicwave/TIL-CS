### 1. What is the order of the language modeling pipeline?

The tokenizer handles text and returns IDs. The model handles these IDs and outputs a prediction. The tokenizer can then be used once again to convert these predictions back to some text.

### 2. How many dimensions does the tensor output by the base Transformer model have, and what are they?

 3: The sequence length, the batch size, and the hidden size

### 3. Which of the following is an example of subword tokenization?

 BPE

### 4. What is a model head? ðŸ¤¯

An additional component, usually made up of one or a few layers, to convert the transformer predictions to a task-specific output

### 5. What is an AutoModel?

 An object that returns the correct architecture based on the checkpoint

### 6. What are the techniques to be aware of when batching sequences of different lengths together?

 Truncating

### 7. What is the point of applying a SoftMax function to the logits output by a sequence classification model? ðŸ¤¯

 It applies a lower and upper bound so that they're understandable.

### 8. What method is most of the tokenizer API centered around?

Calling the tokenizer object directly.

### 9. What does the `result` variable contain in this code sample?

```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```

 A list of strings, each string being a token

### 10. Is there something wrong with the following code? ðŸ¤¯

```
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```

The tokenizer and model should always be from the same checkpoint.