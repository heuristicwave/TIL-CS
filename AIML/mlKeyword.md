## 머신러닝 키워드

**특성** : 데이터를 표현하는 하나의 성질.

**훈련** : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정 (사이킷런에서 `fit()` 메서드의 역할)

**K-최근접 이웃 알고리즘** : 가장 가까운 이웃을 참고해 정답을 예측하는 간단한 머신러닝 알고리즘

**모델** : 머신러닝 알고리즘이 구현된 객체

**정확도** : 맞힌 개수 / 전데 데이터 개수 (사이킷런에서는 0 ~ 1 사이의 값)

**지도 학습** : 입력과 타깃을 전달해 모델을 훈련한 다음 새로운 데이터를 예측하는데 활용

**비지도 학습** : 타깃 데이터가 없음. 무엇을 예측하는 것이 아니라 입력 데이터에서 어떤 특징을 찾는데 활용

**훈련 세트** : 모델 훈련시 사용하는 데이터. 훈련 세트가 클수록 좋음.

**테스트 세트** : 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많음.

**데이터 전처리** : 머신러닝 모델에 데이터를 주입하기 전에 가공하는 단계

**표준 점수** : 훈련 세트의 스케일을 바꾸는 대표적인 방법. 표준점수를 얻으려면 특성의 평균을 빼고 표준편차로 나눈다. 반드시 훈련 세트의 평균과 표준편차로 테스트 세트를 바꿔야 함.

**브로드 캐스팅** : 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장해 수행

**회귀** : 임의의 수치를 예측하는 문제 (타깃값도 임의의 수치가 됨)

**K-최근접 이웃 회귀** : k-최근접 이웃 알고리즘을 사용해 회귀 문제를 품. 가장 가까운 이웃샘플을 찾고 이 샘플들의 타깃값을 평균하여 예측으로 삼는다.

**결정계수(R^2)** : 대표적인 회귀 문제의 성능 측정 도구. 1에 가까울수록 좋고, 0에 가까울수록 성능이 나쁨.

**과대적합** : 모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높을 때 일어남. 모델이 훈련 세트에 너무 집착해서 데이터에 내재된 거시적 패턴을 감지하지 못함.

**과소적합** : 훈련 세트와 테스트 세트 성능이 모두 동일하게 낮거나 테스트 세트 성능이 오히려 더 높을 때 일어남. (더 복잡한 모델을 사용해 훈련 세트에 잘 맞는 모델을 만들어야 함)

**선형 회귀** : 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾는다.(특성이 하나면 직선 방정식)
선형회귀가 찾은 특성과 타깃 사이의 관계는 선형 방정식의 **계수** 또는 **가중치**에 저장된다. 머신러닝에서 종종 가중치는 방정식의 기울기와 절편을 모두 의미함.

**모델 파라미터** : 선형 회귀가 찾은 가중치처럼 머신러닝 모델이 특성에서 학습한 파라미터를 말함.

**다항 회귀** : 다항식을 사용하여 특성과 타깃 사이의 관계를 나타냄. 비선형일수 있지만 여전히 선형 회귀로 표현

**다중 회귀** : 여러 개의 특성을 사용하는 회귀 모델. 특성이 많으면 선형 모델은 강력한 성능 발휘

**특성 공학** : 주어진 특성을 조합하여 새로운 특성을 만드는 일련의 작업 과정

**릿지** : 규제가 있는 선형 회귀 모델 중 하나이며 선형 모델의 계수를 작게 만들어 과대적합을 완화. 비교적 효과가 좋아 널리 사용하는 규제 방법

**라쏘** : 릿지와 달리 계수 값을 아예 0으로 만들 수 있는 또 다른 규제가 있는 선형 회귀 모델

**하이퍼파라미터** : 머신러닝 알고리즘이 학습하지 않는 파라미터. 사전에 사람이 지정해야하며 대표적으로 릿지와 라쏘의 규제 강도 alpha 파라미터가 있다.

**로지스틱 회귀** : 선형 방정식을 사용한 분류 알고리즘. 선형 회귀와 달리 시그모이드, 소프트맥스 함수를 사용해 클래스 확률 출력

**다중 분류** : 타깃 클래스가 2개 이상인 분류 문제. 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용해 클래스를 예측

**시그모이드 함수** : 선형 방정식의 출력을 0과 1 사이의 값으로 압축하며 이진 분류를 위해 사용

**소프트맥스 함수** : 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만듬

**확률적 경사 하강법** : 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘.
샘플을 하나씩 사용하지 않고 여러개를 사용하면 미니배치 경사 하강법. 한번에 전체 샘플을 사용하면 배치 경사 하강법

**손실 함수** : 확률적 경사 하강법이 최적화할 대상입니다. 이진 분류에는 로지스틱(이진 크로스엔트로피) 손실 함수를 다중 분류에는 크로스엔트로피 손실 함수를 사용.
회귀 문제에는 평균 제곱 오차 손실 함수를 사용

**에포크** : 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미. 일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 반복

**결정 트리** : Yes or No에 대한 질문을 이어나가며 정답을 찾아 학습하는 알고리즘. 비교적 예측 과정이해가 쉽고 성능이 뛰어남.

**불순도** : 결정 트리가 최적의 질문을 찾기 위한 기준.

**정보 이득** : 부모 노드와 자식 노드의 불순도 차이. 결정 트리 알고리즘은 정보 이득이 최대화되도록 학습한다.

**가지치기** : 결정트리는 훈련 세트에 과대적합되기 쉬워 결정트리의 성장을 제한

**특성 중요도** : 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타낸 값

**검증 세트** : 하이퍼파리미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어 낸 데이터 세트

**교차 검증** : 훈련 세트를 여러 폴드로 나눠 한 폴드가 검증 세트의 역할을 하고 나머지 폴드에서는 모델을 훈련. 이와 같이 모든 폴드에 대해 검증 점수를 얻어 평균하는 방법

**그리드 서치** : 하이퍼파리미터 탐색을 자동화해 주는 도구. 탐색할 매개변수를 나열하면 교차 검증을 수행해 가장 좋은 점수의 매개변수 조합을 선택하고 최종 모델을 훈련

**랜덤 서치** : 연속된 매개변수 값을 탐색할 때 유용. 탐색 값을 샘플링할 수 있는 확률 분포 객체를 전달. 지정된 횟수만큼 샘플링하여 교차 검증을 수행하기 때문에 시스템 자원이 허락하는 만큼 탐색량을 조절

**앙상블 학습** : 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘

**랜덤 포레스트** : 대표적인 결정 트리 기반의 앙상블 학습 방법. 부트스트랩 샘플을 사용해 랜덤하게 일부 특성을 선택해 트리를 만듬
